Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features (e.g. rahimi2008random,rahimi2009weighted). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that under minimal assumptions, random features cannot be used to learn even a single ReLU neuron (over standard Gaussian inputs in and weights), unless the network size (or magnitude of its weights) is exponentially large in . Since a single neuron with Gaussian inputs is known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials. Deep learning, in the form of artificial neural networks, has seen a dramatic resurgence in popularity in recent years. This is mainly due to impressive performance gains on various difficult learning problems, in fields such as computer vision, natural language processing and many others. Despite the practical success of neural networks, our theoretical understanding of them is still very incomplete. A key aspect of modern networks is that they tend to be very large, usually with many more parameters than the size of the training data: In fact, so many that in principle, they can simply memorize all the training examples (as shown in the influential work of zhang2016understanding). The fact that such huge, over-parameterized networks are still able to learn and generalize is one of the big mysteries concerning deep learning. A current leading hypothesis is that over-parameterization makes the optimization landscape more benign, and encourages standard gradient-based training methods to find weight configurations that fit the training data as well as generalize (even though there might be many other configurations which fit the training data without any generalization). However, pinpointing the exact mechanism by which over-parameterization helps is still an open problem. Recently, a spate of papers (such as andoni2014learning,daniely2017sgd,du2018gradient,allen2018learning,li2018learning,du2018gradient2,cao2019generalization,allen2018convergence,allen2019can) provided positive results for training and learning with over-parameterized neural networks. Although they differ in details, they are all based on the following striking observation: When the networks are sufficiently large, standard gradient-based methods change certain components of the network (such as the weights of a certain layer) very slowly, so that if we run these methods for a bounded number of iterations, they might as well be fixed. To give a concrete example, consider one-hidden-layer neural networks, which can be written as a linear combination of neurons equation. using weights and an activation function . When is sufficiently large, and with standard random initializations, it can be shown that gradient descent will leave the weights in the first layer nearly unchanged (at least initially). As a result, the dynamics of gradient descent will resemble those where are fixed at random initial values -- namely, where we learn a linear predictor (parameterized by ) over a set of random features of the form (for some random choice of ). For such linear predictors, it is not difficult to show that they will converge quickly to an optimal predictor (over the span of the random features). This leads to learning guarantees with respect to hypothesis classes which can be captured well by such random features: For example, most papers focus (explicitly or implicitly) on multivariate polynomials with certain constraints on their degree or the magnitude of their coefficients. We discuss these results in more detail (and demonstrate their close connection to random features) in Section section:analysis of neural networks as random features. Taken together, these results are a significant and insightful advance in our understanding of neural networks: They rigorously establish that sufficient over-parameterization allows us to learn complicated functions, while solving a non-convex optimization problem. However, it is important to realize that this approach can only explain learnability of hypothesis classes which can already be learned using random features. Considering the one-hidden-layer example above, this corresponds to learning linear predictors over a fixed representation (chosen obliviously and randomly at initialization). Thus, it does not capture any element of representation learning, which appears to lend much of the power of modern neural networks. In this paper we show that there are inherent limitations on what predictors can be captured with random features, and as a result, on what can be provably learned with neural networks using the techniques described earlier. We consider random features of the form which are chosen from some fixed distribution. The 's can be arbitrary functions, including multilayered neural networks and kernel functions, as long as their norm (suitably defined) is not exponential in the input dimension. We show that using we cannot efficiently approximate even a single ReLU neuron: In particular, we can choose a target weight vector and bias term , such that if \[ _[(N(x)-[w^*,x+b^*]_+)^2] ~~ 150 \] (where is the ReLU function and has a standard Gaussian distribution) then \[ r_i |u_i| ((d)). \] In other words, either the number of features or the magnitude of the weights (or both) must be exponential in the dimension . Moreover, if the random features can be written as for a random matrix (which includes for instance vanilla neural networks of any size), then the same result holds for any choice of . These results imply that the random features approach cannot fully explain polynomial-time learnability of neural networks, even with respect to data generated by an extremely simple neural network, composed of a single neuron. This is despite the fact that single ReLU neurons with Gaussian inputs are easily learnable with gradient-based methods (e.g., mei2016landscape, soltanolkotabi2017learningTo be precise, existing theoretical results usually ignore the bias term for simplicity, but we believe this is not a real impediment for the analysis.). The point we want to make here is that the random feature approach, as a theory for explaining the success of neural networks, cannot explain even the fact that single neurons are learnable. For completeness we also provide a simple, self-contained analysis, showing how over-parameterized, one-hidden-layer networks can provably learn polynomials with bounded degrees and coefficients, using standard stochastic gradient descent with standard initialization. We emphasize that there is no contradiction between our positive and negative results: In the positive result on learning polynomials, the required size of the network is exponential in the degree of the polynomial, and low-degree polynomials cannot express even a single ReLU neuron if its weights are large enough. Overall, we argue that although the random feature approach captures important aspects of training neural networks, it is by no means the whole story, and we are still quite far from a satisfying general explanation for the empirical success of neural networks. The recent literature on the theory of deep learning is too large to be thoroughly described here. Instead, we survey here some of the works most directly relevant to the themes of our paper. In Section section:analysis of neural networks as random features, we provide a more technical explanation on the connection of recent results to random features. Denote by the -dimensional uniform distribution over the rectangle , and by the multivariate Gaussian distribution with covariance matrix . For let , and for a vector we denote by the norm. We denote the ReLU function by . In many previous works, a key element is to analyze neural networks as if they are random features, either explicitly or implicitly. Here we survey some of these works and how they can actually be viewed as random features. One approach is to fix the output layer and do optimization only on the inner layers. Most works that use this method (e.g. li2018learning, du2018gradient2, allen2018convergence, cao2019generalization, allen2018learning, allen2019can) also use the method of "coupling" and the popular ReLU activation. This method uses the following observation: a ReLU neuron can be viewed as a linear predictor multiplied by a threshold function, that is: . The coupling method informally states that after doing gradient descent with appropriate learning rate and a limited number of iterations, the amount of neurons that change the sign of (for in the data) is small. Thus, it is enough to analyze a linear network over random features of the form: where are randomly chosen. For example, a one-hidden-layer neural network where the activation is the ReLU function can be written as Using the coupling method, after doing gradient descent, the amount of neurons that change sign, i.e. the sign of changes, is small. As a result, using the homogeneity of the ReLU function, the following network can actually be analyzed: where are randomly chosen. This is just analyzing a linear predictor with random features of the form . Note that the homogeneity of the ReLU function is used in order to show that fixing the output layer does not change the network's expressiveness. This is not true in terms of optimization, as optimizing both the inner layers and the output layers may help the network converge faster, and to find a predictor which has better generalization properties. Thus, the challenge in this approach is to find functions or distributions that can be approximated with this kind of random features network, using a polynomial number of features. A second approach in the literature (e.g. andoni2014learning, daniely2016toward, du2018gradient) is to perform optimization on all the layers of the network, choose a "good" learning rate and bound the number of iterations such that the inner layers stay close to their initialization. For example, in the setting of a one-hidden-layer network, for every , a learning rate and number of iterations are chosen, such that after running gradient descent with these parameters, there is an iteration such that: Hence, it is enough to analyze a linear predictor over a set of random features: where is not necessarily the ReLU function. Again, the difficulty here is finding the functions that can be approximated in this form, where (the amount of neurons) is only polynomial in the relevant parameters. For completeness we provide a simple, self-contained analysis, showing how over-parameterized, one-hidden-layer networks can provably learn polynomials with bounded degrees and coefficients, using standard stochastic gradient descent with standard initialization. The data for our network is , drawn from an unknown distribution . We assume for simplicity that and . We consider one-hidden-layer feed-forward neural networks which are defined as: where is an activation function which acts coordinate-wise and . We will also use the following form for the network: equation. here and . For simplicity we will use the hinge loss, which is defined by: , thus the optimization will be done on the function . We will also use the notation: We will use the standard form of SGD to optimize , where at each iteration a random sample is drawn from and we update: The initialization of is a standard Xavier initialization glorot2010understanding, that is . can be initialized in any manner, as long as its norm is smaller than , e.g. we can initialize = 0. This kind of initialization for the outer layer has been used also in other works (see daniely2017sgd, andoni2014learning). The main result of this section is the following: Let be the inputs dimension. For some , let be an analytic activation function with Taylor coefficients , which is -Lipschitz with and satisfies that for every . Let be any distribution over the labelled data with , and let , , and be some positive integer. Suppose we run SGD on the neural network: with the following parameters: where and . Then, for every polynomial with , the coefficients of are bounded by and all the monomials of which have a non-zero coefficient also have a non-zero coefficient in the Taylor series of , w.p over the initialization there is such that: Here the expectation is over the random choice of in each iteration of SGD. We note that for simplicity, we focused on analytic activation functions, although it is possible to derive related results for non-analytic activations such as a ReLU (see Appendix app:relu for a discussion). The assumptions on the Taylor coefficients of the activation function includes for example the and activations. We note that similar assumptions were used also in other works on learning polynomials (e.g. ghorbani2021linearized). Also, note that we did not use a bias term in the architecture of the network in the theorem (namely, we have and not ). This is because if the polynomial we are trying to compete with has a constant factor, then we require that the Taylor expansion of the activation also has a constant factor, thus the bias term is already included in the Taylor expansion of the activation function. Suppose we are given a sample set . By choosing uniform on the sample set , Theorem Main theorem of the paper with analytic activation shows that SGD over the sample set will lead to an average loss not much worse than the best possible polynomial predictor with bounded degree and coefficients. At high level, the proof idea of Theorem Main theorem of the paper with analytic activation is divided into three steps. In the first step we show that with an appropriate learning rate and limited amount of iterations, neural networks generalize better than random features. This step allows us to focus our analysis on the behaviour of a linear combination of random features instead of the more complicated architecture of neural networks. In the second step using McDiarmid's theorem we show that by taking enough random features, they concentrate around their expectation. In the third step we use Legendre's polynomials to show that any polynomial can be approximated by an expectation of random features. First we introduce some notations regarding multi-variable polynomials: Letting be a multi index, and given , we define , and also . We say for two multi indices that if for all , and that if and also there is an index such that . For and multi index we say that if . Lastly, given a multi-variable polynomials , where we define: We break the proof to three steps, where each step contains a theorem which is independent of the other steps. Finally we combine the three steps to prove the main theorem. Recall we use a network of the form: equation. where are initialized as described in the theorem We show that for any target matrix with a small enough norm and every , if we run SGD on with appropriate learning rate and number of iterations , there is some with: equation. where the expectation is over the random choices of examples in each round of SGD. The bound in outline generalization bound U t W t U * W 0 means that SGD on randomly initialized weights competes with random features. By random features here we mean any linear combination of neurons of the form where the are randomly chosen, and the norm of the weights of the linear combination are bounded. In more details: Assume we initialize such that and . Also assume that is -Lipschitz with , and let be a constant. Letting , we run SGD with step size and steps with and let be the weights produced at each step. If we pick such that then for every target matrix with there is a s.t: \[ E[L_D(W_t, U_t)] L_D(W_0,U^*) + . \] Here the expectation is over the random choice of the training examples in each round of SGD. In the proof of Theorem theorem about optimization bound related to W 0 U 0 we first show that for the chosen learning rate and limited number of iterations , the matrix does not change much from its initialization. After that we use results from online convex optimization for linear prediction with respect to with a sufficiently small norm to prove the required bound. For a full proof see Appendix appendix optimization with respect to a target matrix. Note that in the theorem we did not need to specify the initialization scheme, only to bound the norm of the initialized weights. The optimization analysis is similar to the one done in Daniely daniely2017sgd. In the previous step we showed that in order to bound the expected loss of the network, it is enough to consider a network of the form , where the are randomly initialized with . We now show that if the number of random features is large enough, then a linear combination of them approximates functions of the form for an appropriate normalization factor : Let where is -Lipschitz on with , and a normalization term. Assume that for a constant . Then for every if are drawn i.i.d from the uniform distribution on , w.p there is a function of the form \[ f(x) = _i=1^ru_i (w_i,x) \] where for every , such that: \[ _x |f(x) - f(x) | LCr(4 + 2 (1)) \] Theorem theorem about approximating integral as a sum basically states that random features concentrate around their expectation, and the rate of convergence is where is the amount of random features that were sampled. The proof is based on concentration of measure and Rademacher complexity arguments, and appears in Appendix appendix approximating integral to a discrete sum. In the previous step we showed that random features can approximate functions with the integral form: In this step we show how a a polynomial with bounded degree and coefficients can be approximated by this form. This means that we need to find a function for which . To do so we use the fact that is analytic, thus it can be represented as an infinite sum of monomials using a Taylor expansion, and take to be a finite weighted sum of Legendre polynomials, which are orthogonal with respect to the appropriate inner product. The main difficulty here is to find a bound on , which in turn also bounds the distance between the sum of the random features and its expectation. The main theorem of this step is: For some , let be an analytic function with Taylor expansion coefficients which satisfies for every . Let be a polynomial, where , and all the monomials of which have non-zero coefficient also have a non-zero coefficient in the Taylor series of . Assume , for some ,and denote . Then, for any there exists a function that satisfies the following: integral representation of sum of powers bound on g(w) where is a normalization term. For a full proof of Theorem theorem about integral representation of polynomials using legendre and an overview of Legendre polynomials see Appendix appendix about representing polynomials in an integral form. We are now ready to prove the main theorem of this section. The proof is done for convenience in reverse order of the three steps presented above. Let be the coefficients of the Taylor expansion of up to degree , and let be a a polynomial with and , such that if then the monomials in of degree also have a zero coefficient. First, we use Theorem theorem about integral representation of polynomials using legendre to find a function such that: equation. Then we consider drawing random features i.i.d. Using Theorem theorem about approximating integral as a sum, the choice of and main theorem polynomial as an integral, w.p there is such that: &= _x 1 | _i=1^r u_i(w_i,x) - c_d_w [-1d,1d]^d(w,x) g(w)| , and also , thus . Finally, we use Theorem theorem about optimization bound related to W 0 U 0 with the defined learning rate and iterations to find such that: equation. Combining main theorem polynomial as an integral, discrete sum and polynomial are close with almost final approximation, only neede the polynomial gives: Re-scaling finishes the proof. Having discussed and shown positive results for learning using (essentially) random features, we turn to discuss the limitations of this approach. Concretely, we will consider in this section data , where is drawn from a standard Gaussian on , and there exists some single ground-truth neuron which generates the target values : Namely, for some fixed . We also consider the squared loss , so the expected loss we wish to minimize takes the form equation. where are the random features. Importantly, when , is the ReLU function, and (that is, we train a single neuron with Gaussian inputs to learn a single target neuron), this problem is quite tractable with standard gradient-based methods (see, e.g., mei2016landscape, soltanolkotabi2017learning). In this section, we ask whether this positive result -- that single target neurons can be learned -- can be explained by the random features approach. Specifically, we consider the case where the function are arbitrary functions chosen obliviously of the target neuron (e.g. multilayered neural networks at a standard random initialization), and ask what conditions on and are required to minimize eq:targetsquared. Our main results (Theorem thm:limitations of random features and Theorem thm:limitations of random features with coupling) show that either one of them has to be exponential in the dimension , as long as the sizes of are allowed to be polynomial in . Since networks with exponentially-many neurons or exponentially-sized weights are generally not efficiently trainable, we conclude that an approach based on random-features cannot explain why learning single neurons is tractable in practice. In Theorem thm:limitations of random features, we show the result for any choice of with some fixed norm, but require the feature functions to have a certain structure (which is satisfied by neural networks). In Theorem thm:limitations of random features with coupling, we drop this requirement, but then the result only holds for a particular . To simplify the notation in this section, we consider functions on as elements of the space weighted by a standard Gaussian measure, that is &\|f(x)\|^2 := E_x[f^2(x)]= c_d_R^d f^2(x)e^-\|x\|^22dx,  & f(x),g(x) := E_x[f(x)g(x)] = c_d_R^d f(x)g(x)e^-\|x\|^22dx, where is a normalization term. For example, eq:targetsquared can also be written as . Before stating our main results, let us consider a particularly simple case, where is the identity, and our goal is to learn a linear predictor with . We will show that already in this case, there is a significant cost to pay for using random features. The main result in the next subsection can be seen as an elaboration of this idea. In this setting, finding a good linear predictor, namely minimizing is easy: It is a convex optimization problem, and is easily solved using standard gradient-based methods. Suppose now that we are given random features and want to find such that: equation. The following proposition shows that with high probability, linear predictor random features cannot hold unless . This shows that even for linear predictors, there is a price to pay for using a combination of random features, instead of learning the linear predictor directly. Let be some unit vector in , and suppose that we pick random features i.i.d. from any spherically symmetric distribution in . If , then with probability at least (for some universal constant ), for any choice of weights , it holds that .\] The full proof appears in Appendix appendix proofs from section limitations of random features, but the intuition is quite simple: With random features, we are forced to learn a linear predictor in the span of , which is a random -dimensional subspace of . Since this subspace is chosen obliviously of , and , it is very likely that is not close to this subspace (namely, the component of orthogonal to this subspace is large), and therefore we cannot approximate this target linear predictor very well. Let with , and suppose we pick random features with . Then for all we have: Let with and let . Then w.p over sampling of (where are distributed as , if there are such that: then: We note that in this setting, it is possible to prove the result without the restriction on . However, we chose to present the result in this manner, so that the proof will be more similar to the one we employ in the non-linear case later on (we conjecture that removing the dependence on in the non-linear case is possible, and is left to future work). Having discussed the linear case, let us return to the case of a non-linear neuron. Specifically, we will show that even a single ReLU neuron cannot be approximated by a very large class of random feature predictors, unless the amount of neurons in the network is exponential in the dimension, or the coefficients of the linear combination are exponential in the dimension. In more details: There exist universal constants such that the following holds. Let , , and let be a family of functions from to . Also, let be a random matrix whose rows are sampled uniformly at random from the unit sphere. Suppose that satisfies for any realization of and for all . Then there exists with such that for every with , and for any , w.p over sampling , if then There exist universal constants such that the following holds. Let , let and be a family of functions. For and denote . Suppose that after sampling each row of as , then (w.h.p over sampling of ) for all : . Then for every with , and for all , w.p over sampling of there exists with such that if: then: Note that the theorem allows any ``random'' feature which can be written as a composition of some function (chosen randomly or not), and a random linear transformation. This includes as special cases one-hidden layer networks (, with each chosen randomly), or multi-layer neurons of any depth (as long as the first layer performs a random linear transformation). As opposed to the linear case, here we also have a restriction on . We conjecture that it is possible to remove this dependence and leave it to future work. To prove the theorem, we will use the following proposition, which implies that functions of the form for a certain sine-like and "random" are nearly uncorrelated with any fixed function. Let , where for some positive constant , and let . Define the following function: Then satisfies the following: . , where is sampled uniformly from , and is a universal constant. Items first item of psi theorem and second item of psi theorem follow by a straightforward calculation, where in item second item of psi theorem we also used the fact that has a symmetric distribution. Item third item of psi theorem relies on a claim from shamir2018distribution, which shows that periodic functions of the form for a random with sufficiently large norm have low correlation with any fixed function. The full proof can be found in Appendix appendix proofs from section limitations of random features. At a high level, the proof of Theorem thm:limitations of random features proceeds as follows: If we choose and fix and , then any linear combination of random features with small weights will be nearly uncorrelated with , in expectation over . But, we know that can be written as a linear combination of ReLU neurons, so there must be some ReLU neuron which will be nearly uncorrelated with any linear combination of the random features (and as a result, cannot be well-approximated by them). Finally, by a symmetry argument, we can actually fix arbitrarily and the result still holds. We now turn to provide the formal proof: Take from Proposition proposition properties of psi and denote for , . If we sample uniformly from , then for all : E_w^* [| f_W,_w^*|] 20\|f_W\|^2(-cd) (-c_3 d), where is a universal constant that depends only on the constant from Proposition proposition properties of psi and on . Hence also: Therefore, there is with such that: equation. Using Markov's inequality and dividing by a factor of , we get w.p over sampling of , $ |f_W,_w^*| (-c_3d) w^*$ we found above. Finally, if we pick , using the union bound we get that w.p over sampling of : We can write , where and , for . Let with be as above, and denote . Assume that for every we can find and such that , where is distributed as above. Let the bias term of the output layer of the network, then also: &\|_i=0^r(_j=1^au_i^ja_j)f_i(Wx) - _j=1^a a_jf^*_j(x) - 1\|^2 = \|_i=0^ru_if_i(Wx) - (w^*,x)\|^2  where . On the other hand using bound w.h.p on <f i , psi> and item second item of psi theorem from Proposition proposition properties of psi we get w.p over the distribution of that: & \|_i=0^ru_if_i(Wx) - (w^*,x)\|^2 \|_w^*\|^2 - 2|_i=0^r u_if_i_W,_w^*|  &16 - 2_i|u_i| _i=1^r |f_i_W,_w^*| 16 - 2_i|u_i| r (-c_3 d)  & 16 - 4a_i|u^j_i| r (-c_3 d), where the last inequality is true for all . Combining upper bound on the norm between psi and random features combination and lower bound on on the norm between psi and random features combination, there are with and such that w.p over the sampling of , if there is that: equation. then . We now show that the above does not depend on . Any with can be written as for some orthogonal matrix . Now: &\|_i=1^r u_i f_i(Wx) - [w^*,x + b^*]_+ \| = \|_i=1^r u_i f_i(WM^ Mx) - [Mw^*,Mx + b^*]_+ \| & = E_x[_i=1^r u_i f_i(WM^ Mx) - [Mw^*,Mx + b^*]_+ ] = E_x[_i=1^r u_i f_i(WM^ x) - [Mw^*,x + b^*]_+ ]  & = \|_i=1^r u_i f_i(WM^ x) - [Mw^*,x + b^*]_+ \|, where we used the fact that have a spherically symmetric distribution. We also use the fact that the rows of have a spherically symmetric distribution to get the same probability as above, thus we can replace by to get the following: There exists such that for all with , w.p over the sampling of , if there is that: equation. then . Lastly, by multiplying both sides of upper bound by epsilon on a single relu by , using the homogeneity of ReLU and setting we get that there is such that for every with , w.p over the sampling of , if there is such that then . It is possible to trade-off between the norm of and the required error. This can be done by altering the proof of Theorem thm:limitations of random features, where instead of multiplying both sides of upper bound by epsilon on a single relu by we could have multiplied both sides by . This way the following is proved: With the same assumptions as in Theorem thm:limitations of random features, for all with there is with such that for all and all , w.h.p over sampling of , if , then for a universal constant . In the previous subsection, we assumed that our features have a structure of the form for a random matrix . We now turn to a more general case, where we are given features of any kind without any assumptions on their internal structure, as long as they are sampled from some fixed distribution. Besides generalizing the setting of the previous subsection, it also captures the setting of Subsection subsec:optimization with coupling, where the coupling method is used in order to show that equation. This is because it is a weighted sum of the random features where are initialized by standard Xavier initialization. Moreover, in this general setting, we also capture kernel methods, since kernel predictors can be seen as linear combinations of features of the form where are elements of the sampled training data. We show that even in this general setting, it is impossible to approximate single ReLU neurons in the worst-case (at the cost of proving this for some target weight vector , instead of any ). There exist universal constants such that the following holds. Let , and let be a family of functions from to , such that for all . Also, for some , let be an arbitrary distribution over tuples of functions from . Then there exists with , and with , such that with probability at least over sampling , if then The proof is similar to the proof of Theorem thm:limitations of random features. The main difference is that we do not have any assumptions on the distribution of the random features (as the assumption on the distribution on in Theorem thm:limitations of random features), hence we can only show that there exists some ReLU neuron that cannot be well-approximated. On the other hand, we have almost no restrictions on the random features, e.g. they can be multi-layered neural network of any architecture and with any random initialization, kernel-based features on sampled training sets, etc. This research is supported in part by European Research Council (ERC) grant 754705. We thank Yuanzhi Li for some helpful comments on a previous version of this paper, and for Pritish Kamath and Alex Damian for spotting a bug in a previous version of the paper.